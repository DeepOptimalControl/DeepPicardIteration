import abc
import pathlib
import warnings
from collections import namedtuple
from math import ceil
from typing import Union, List, Optional

import lightning as pl
import numpy as np
import psutil
import torch
from lightning.fabric.utilities.warnings import PossibleUserWarning
from lightning.pytorch.utilities.types import EVAL_DATALOADERS
from rich import print as rprint
from rich.panel import Panel
from sympy import divisors
from torch.utils.data import (
    DataLoader,
    get_worker_info,
    TensorDataset,
    Dataset,
    IterableDataset,
)
from yacs.config import CfgNode

from picard.config import (
    set_default_dtype,
    get_standard_float_type_np,
    get_float_type_bytes,
)
from picard.data_saver import H5Saver
from picard.dataset import (
    IterableDatasetWithInternalBatch,
    CacheToFileWrapper,
    CacheToMemoryWrapper,
    DummyDataset,
)
from picard.equations import (
    Equation,
    SimpleDiffusionEquation,
    SimpleDiffusionEquationWithHessian,
    OUProcessEquation,
)
from picard.memory import GPUMemoryTracker
from picard.utils import (
    get_device_prefer_cpu,
    get_device_prefer_cuda,
    hutchinson_trace_estimation_batch,
    get_laplacian,
)


class DataGenerator:
    """
    The data generator generates dataset with the same signature:
     - n_total: total number of data to generate
     - n_batch_buffer: number of batches to generate in each call to batch_data_generator.
        A large number will consume more memory and introduce more latency.
        A small number will incur the vectorization performance.
    - batch_size: batch size
        the internal batch size of the data generated by the dataset.
        can be ignored when do_internal_batching is False, i.e., let DataLoader do the batching.
    """

    do_internal_batching = True

    def dataset(self, n_total: int, n_batch_buffer: float, batch_size: int) -> Dataset:
        raise NotImplementedError

    def dataset_with_gradients(self, n_total: int, n_batch_buffer: float, batch_size: int) -> Dataset:
        raise NotImplementedError

    def dataset_exact(self, n_total: int, n_batch_buffer: float, batch_size: int) -> Dataset:
        raise NotImplementedError

    def dataset_exact_with_gradients(self, n_total: int, n_batch_buffer: float, batch_size: int) -> Dataset:
        raise NotImplementedError

    def dataset_with_gradients_and_hessians(self, n_total: int, n_batch_buffer: float, batch_size: int) -> Dataset:
        raise NotImplementedError

    def dataset_exact_with_gradients_and_hessians(
        self, n_total: int, n_batch_buffer: float, batch_size: int
    ) -> Dataset:
        raise NotImplementedError


class _OnlineDataGenerator(DataGenerator):
    def __init__(
        self,
        equation: Equation,
        N: int,
        i: int,
        *,
        device: Union[str, torch.device] = torch.device("cpu"),
        t_always_uniform=False,
        hessian_approximation=None,
        sample_bound=None,
        estimate_terminal=None,
        estimate_integral=None,
        estimate_delta_t=0.0,
    ):
        self.equation = equation
        self.N = N
        self.i = i

        self.T = torch.scalar_tensor(equation.T)
        self._device = get_device_prefer_cpu(device)
        if t_always_uniform:
            self.t_sampler = self.sample_t_always_uniform
        else:
            self.t_sampler = self.sample_t
        self.hessian_approximation_ctx = (
            {"config": hessian_approximation} if hessian_approximation.method is not None else None
        )
        self.hessian_approximation_config = hessian_approximation
        if self.hessian_approximation_ctx is not None:
            print(f"Hessian approximation is enabled with {hessian_approximation}")
            assert (
                hessian_approximation.method in self.equation.supported_approximate_methods
            ), f"Current equation does not support the method {hessian_approximation.method}"

        if sample_bound is not None:
            self.sample_bound = torch.tensor(sample_bound, device=self._device)
            print(f"Sample bound is enabled with {sample_bound}")
        else:
            # sample bound is torch infinite
            self.sample_bound = torch.tensor(float("inf"), device=self._device)

        self.estimate_terminal_type = estimate_terminal
        self.estimate_integral_type = estimate_integral
        self.estimate_delta_t = estimate_delta_t

        if "ByGx" in self.estimate_terminal_type or "Joint" in self.estimate_integral_type:
            self.generate_t_uniform_epsilon = torch.tensor(0.01, device=self._device)
        else:
            self.generate_t_uniform_epsilon = torch.tensor(0.0, device=self._device)

    def to(self, device):
        self._device = get_device_prefer_cpu(device)
        self.equation = self.equation.to(device=device)
        self.T = self.T.to(device=device)
        return self

    @property
    def device(self):
        return self._device

    def sample_t(self, n_batch: int):
        """
        the distribution of t is: T[1-U[0,1]**(N-i+1)]
        :param n_batch: number of samples
        :return:
        """
        # first sample (n_batch, N-i+1) from U[0,1]
        t = torch.rand(n_batch, self.N - self.i + 1, device=self._device)
        # then do multiplication
        t = torch.prod(t, dim=-1, keepdim=True)
        return self.T * (1 - t)

    def sample_t_always_uniform(self, n_batch: int):
        """
        :param n_batch: number of samples
        :return:
        """
        t = torch.rand(n_batch, 1, device=self._device)
        return (self.T - 2 * self.generate_t_uniform_epsilon) * (1 - t) + self.generate_t_uniform_epsilon

    @abc.abstractmethod
    def generate(self, tx: torch.Tensor) -> torch.Tensor:
        """
        Estimate u(t, x) for u to learn.
        :param tx:
        :return:
            (tx.size(0), 1)
        """

    @abc.abstractmethod
    def generate_with_gradients(self, tx: torch.Tensor) -> torch.Tensor:
        """
        Estimate u(t, x) for u to learn.
        :param tx:
        :return:
            (tx.size(0), 1+nx)
        """

    @abc.abstractmethod
    def generate_with_gradients_and_hessians(self, tx: torch.Tensor) -> torch.Tensor:
        """
        Estimate u(t, x) for u to learn.
        :param tx:
        :return:
            (tx.size(0), 1+nx+nx*nx)
        """

    def sample(self, n_batch):
        """
        :param n_batch:
        :return:
            tx: (n_batch, 1+nx)
            u: (n_batch, 1)
        """
        with torch.no_grad():
            t = self.t_sampler(n_batch)
            x = self.equation.sample_x(t)
            tx = torch.cat([t, x], dim=-1)
            u = self.generate(tx)
        u = torch.clip(u, -self.sample_bound, self.sample_bound)
        return tx, u

    def sample_with_gradients(self, n_batch):
        """
        :param n_batch:
        :return:
            tx: (n_batch, 1+nx)
            u_ux: (n_batch, 1+nx)
        """
        t = self.t_sampler(n_batch)
        x = self.equation.sample_x(t)
        tx = torch.cat([t, x], dim=-1)
        u_ux = self.generate_with_gradients(tx).detach()
        u_ux = torch.clip(u_ux, -self.sample_bound, self.sample_bound)
        return tx, u_ux

    def sample_with_gradients_and_hessians(self, n_batch):
        """
        :param n_batch:
        :return:
            tx: (n_batch, 1+nx)
            u_ux_uxx: (n_batch, 1+nx+nx*nx)
        """
        t = self.t_sampler(n_batch)
        x = self.equation.sample_x(t)
        tx = torch.cat([t, x], dim=-1)
        u_ux_uxx = self.generate_with_gradients_and_hessians(tx)
        u_ux_uxx = torch.clip(u_ux_uxx, -self.sample_bound, self.sample_bound)
        return tx, u_ux_uxx

    def sample_exact(self, n_batch):
        """
        :param n_batch:
        :return:
            tx: (n_batch, 1+nx)
            u: (n_batch, 1)
        """
        with torch.no_grad():
            t = self.t_sampler(n_batch)
            x = self.equation.sample_x(t)
            u = self.equation.exact_solution(t, x)
        return torch.concat([t, x], dim=-1), u

    def sample_exact_with_gradients(self, n_batch):
        """
        :param n_batch:
        :return:
            tx: (n_batch, 1+nx)
            u_ux: (n_batch, 1+nx)
        """
        with torch.no_grad():
            t = self.t_sampler(n_batch)
            x = self.equation.sample_x(t)
            u, ux = self.equation.u_u_x(t, x)
        return torch.concat([t, x], dim=-1), torch.concat([u, ux], dim=-1)

    def sample_exact_with_gradients_and_hessians(self, n_batch):
        """
        :param n_batch:
        :return:
            tx: (n_batch, 1+nx)
            u_ux_uxx: (n_batch, 1+nx+nx*nx)

        """
        # Note: Only SimpleDiffusionEquationWithHessian is supported.
        assert isinstance(self.equation, SimpleDiffusionEquationWithHessian)

        with torch.no_grad():
            t = self.t_sampler(n_batch)
            x = self.equation.sample_x(t)
            u, ux, u_hessian = self.equation.u_u_x_u_hessian(t, x)
        return torch.concat([t, x], dim=-1), torch.concat(
            [u, ux, u_hessian.reshape(u.shape[0], self.equation.nx * self.equation.nx)],
            dim=-1,
        )

    def dataset(self, n_total: int, n_batch_buffer: int, batch_size: int) -> IterableDatasetWithInternalBatch:
        """
        Generate the IterableDatasetWithInternalBatch.
        """
        return IterableDatasetWithInternalBatch(n_total, n_batch_buffer, batch_size, self.sample)

    def dataset_with_gradients(
        self, n_total: int, n_batch_buffer: int, batch_size: int
    ) -> IterableDatasetWithInternalBatch:
        """
        Generate the IterableDatasetWithInternalBatch.
        """
        return IterableDatasetWithInternalBatch(n_total, n_batch_buffer, batch_size, self.sample_with_gradients)

    def dataset_with_gradients_and_hessians(
        self, n_total: int, n_batch_buffer: int, batch_size: int
    ) -> IterableDatasetWithInternalBatch:
        """
        Generate the IterableDatasetWithInternalBatch.
        """
        return IterableDatasetWithInternalBatch(
            n_total, n_batch_buffer, batch_size, self.sample_with_gradients_and_hessians
        )

    def dataset_exact(self, n_total: int, n_batch_buffer: int, batch_size: int) -> IterableDatasetWithInternalBatch:
        """
        Generate the IterableDatasetWithInternalBatch.
        """
        return IterableDatasetWithInternalBatch(n_total, n_batch_buffer, batch_size, self.sample_exact)

    def dataset_exact_with_gradients(
        self, n_total: int, n_batch_buffer: int, batch_size: int
    ) -> IterableDatasetWithInternalBatch:
        """
        Generate the IterableDatasetWithInternalBatch.
        """
        return IterableDatasetWithInternalBatch(n_total, n_batch_buffer, batch_size, self.sample_exact_with_gradients)

    def dataset_exact_with_gradients_and_hessians(
        self, n_total: int, n_batch_buffer: int, batch_size: int
    ) -> IterableDatasetWithInternalBatch:
        """
        Generate the IterableDatasetWithInternalBatch.
        """
        return IterableDatasetWithInternalBatch(
            n_total,
            n_batch_buffer,
            batch_size,
            self.sample_exact_with_gradients_and_hessians,
        )

    @staticmethod
    def repeat_and_split(tx: torch.Tensor, n_repeat: int, nx: int):
        """
        the repeated txR can be reshaped as (n_batch, n_repeat, 1+nx) and [:, i, :] are the same for all i.
        :param tx: (n_batch, 1+nx)
        :param n_repeat:
        :return: (n_batch*n_repeat, 1+nx)
        """
        # add repeat tx => (n_batch*n_estimate, 1+nx)
        # torch.repeat_interleave(torch.unsqueeze(tx, dim=1), n_estimate, dim=1).resize(n_batch*n_batch,-1)
        tx = torch.repeat_interleave(tx, n_repeat, dim=0)
        t, x = torch.narrow(tx, dim=-1, start=0, length=1), torch.narrow(tx, dim=-1, start=1, length=nx)
        return t, x

    def generate_sx_for_integral(self, tx: torch.Tensor, n_multiples: int, return_dW=False):
        """
        :param tx:
        :param n_multiples:
        :param return_dW: whether to return dW
        :return:
        """
        t, x = self.repeat_and_split(tx, n_multiples, self.equation.nx)
        # sample s from uniform distribution on [t, T]
        s = torch.rand_like(t) * (self.T - t) + t
        Xs = self.equation.sample_x_ts(t, s, x, return_dW=return_dW)
        if return_dW:
            Xs, dW = Xs
            sXs = torch.cat([s, Xs], dim=-1)
            return t, s, Xs, sXs, dW
        sXs = torch.cat([s, Xs], dim=-1)
        return t, s, Xs, sXs


class OnlineDataGenerator(_OnlineDataGenerator):
    def __init__(
        self,
        equation: Equation,
        solution: torch.nn.Module,
        N: int,
        i: int,
        *,
        device: Union[str, torch.device] = torch.device("cpu"),
        t_always_uniform=False,
        n_estimate_terminal=1,
        n_estimate_integral=1,
        hessian_approximation=None,
        sample_bound=None,
        estimate_terminal=None,
        estimate_integral=None,
        estimate_delta_t=0.0,
    ):
        """
        :param equation:
        :param solution:
        :param N:
        :param i:
        :param device: device to generate data, by default, it is the cpu
        :param n_estimate_terminal:
        :param n_estimate_integral:
        """
        super().__init__(
            equation,
            N,
            i,
            device=device,
            t_always_uniform=t_always_uniform,
            hessian_approximation=hessian_approximation,
            sample_bound=sample_bound,
            estimate_terminal=estimate_terminal,
            estimate_integral=estimate_integral,
            estimate_delta_t=estimate_delta_t,
        )

        self.solution = solution
        for param in self.solution.parameters():
            param.requires_grad = False
        self.solution.eval()

        self.to(self._device)

        self.n_estimate_terminal = n_estimate_terminal
        self.n_estimate_integral = n_estimate_integral

        self.estimate_terminal_type = estimate_terminal
        self.estimate_integral_type = estimate_integral

        self.estimate_delta_t = estimate_delta_t

        self.solution_output_dim = solution(torch.zeros(1, 1 + equation.nx, device=self.device)).size(1)

        # if equation is not SimpleDiffusionEquation or OUProcessEquation, raise an error
        assert isinstance(self.equation, SimpleDiffusionEquation) or isinstance(
            self.equation, OUProcessEquation
        ), "Currently only SimpleDiffusionEquation and OUProcessEquation are supported"

        assert self.equation.nu == 1, "Currently only nu=1 is supported"

    def to(self, device):
        super().to(device=device)
        self.solution = self.solution.to(device=device)
        return self

    def estimate_terminal(self, tx: torch.Tensor):
        """
        estimate Eg(X_(t,T,x))
        :param tx: (n_batch, 1+nx)
        :return:
        """
        n_estimate = self.n_estimate_terminal
        n_batch = tx.size(0)
        t, x = self.repeat_and_split(tx, n_estimate, self.equation.nx)
        T = torch.ones_like(t) * self.T
        X = self.equation.sample_x_ts(t, T, x)
        g = self.equation.g(X)  # (n_batch*n_estimate, 1)
        g = g.view(n_batch, n_estimate, 1)
        g = torch.sum(g, dim=1, keepdim=False) / n_estimate
        return g

    def estimate_integral(self, tx: torch.Tensor):
        r"""
        estimate E\int_t^T f(s, Xs, u(s,Xs)) ds
        :param tx:
        :return:
        """
        n_estimate = self.n_estimate_integral
        n_batch = tx.size(0)
        t, s, Xs, sXs = self.generate_sx_for_integral(tx, n_estimate)
        u = self.solution(sXs)[:, :1]
        f = self.equation.f(s, Xs, u)

        integral = f * (self.T - t)
        integral = integral.view(n_batch, n_estimate, self.equation.nu)
        integral = torch.sum(integral, dim=1, keepdim=False) / n_estimate
        return integral

    def estimate_integral_with_gradients(self, tx: torch.Tensor):
        r"""
        We make the assumptions:
          - Sigma = sqrt(alpha) I,
          - D = I.
        The second assumption can be derived by assuming that `\mu_x` and `\Sigma_x` are zeros.
        Therefore, the assumptions boil down to:
          - Sigma = sqrt(alpha) I,
          - `\mu_x` = 0.
        For now, SimpleDiffusionEquation satisfies the above assumptions (actually, it further requires \mu=0).
        In the future, we shall move the computation of Ys to the equation class.
        ---
        The formula:
            v(t, x) = \int_t^T f(s, Xs, u(s,Xs), Sigma^T u_x(s,Xs)) (1, Ys) ds
            Ys = Sigma^T/(s-t)\int_t^s [Sigma^{-1}D_{s,r,x}]^T dWr
               = 1/(s-t)\int_t^s dWr
               = 1/sqrt(s-t) N(0,1)^d
        :param tx:
        :return:
            v(t, x): (n_batch, 1 + nx)
        """
        assert isinstance(self.equation, SimpleDiffusionEquation) or isinstance(self.equation, OUProcessEquation)
        n_estimate = self.n_estimate_integral
        n_batch = tx.size(0)
        # we need to compute the gradient of u with respect to Xs
        t, s, Xs, sXs, dW = self.generate_sx_for_integral(tx, n_estimate, return_dW=True)
        if self.hessian_approximation_ctx is not None:
            assert self.hessian_approximation_config.method == "SDGD"
            v = self.hessian_approximation_config.kwargs["v"]
            # indices = torch.multinomial(torch.ones_like(Xs), v)  # (n_batch, v)
            indices = torch.randint(0, Xs.size(1), (Xs.size(0), v), device=Xs.device)
            self.hessian_approximation_ctx["indices"] = indices

        Xs.requires_grad_()

        f = get_f(self.equation, self.solution, Xs, s, self.hessian_approximation_ctx, baseline_repeat=None)

        x_baseline = tx[:, 1:]
        x_baseline.requires_grad_()
        t_baseline = tx[:, 0:1]
        f_baseline = get_f(
            self.equation,
            self.solution,
            x_baseline,
            t_baseline,
            self.hessian_approximation_ctx,
            baseline_repeat=n_estimate,
        )

        Ys = dW / torch.sqrt(s - t) / self.equation.alpha_sqrt
        # shape of eYs: (n_batch*n_estimate, 1 + nx)
        eYs = torch.cat([torch.ones_like(s), Ys], dim=-1)
        integral = (self.T - t) * (f - f_baseline) * eYs
        integral[:, 0:1] = integral[:, 0:1] + f_baseline * (self.T - t)
        integral = integral.view(n_batch, n_estimate, -1)
        integral = torch.sum(integral, dim=1, keepdim=False) / n_estimate
        return integral

    def estimate_integral_with_gradients_td(self, tx: torch.Tensor):
        r"""
        replace T with t_next
        """
        assert isinstance(self.equation, SimpleDiffusionEquation) or isinstance(self.equation, OUProcessEquation)
        n_estimate = self.n_estimate_integral
        n_batch = tx.size(0)
        # we need to compute the gradient of u with respect to Xs

        t, x = self.repeat_and_split(tx, n_estimate, self.equation.nx)
        t_next = torch.clip(t + self.estimate_delta_t, max=self.T)
        # sample s from uniform distribution on [t, t_next]
        s = torch.rand_like(t) * (t_next - t) + t
        Xs = self.equation.sample_x_ts(t, s, x, return_dW=True)
        Xs, dW = Xs

        if self.hessian_approximation_ctx is not None:
            assert self.hessian_approximation_config.method == "SDGD"
            v = self.hessian_approximation_config.kwargs["v"]
            # indices = torch.multinomial(torch.ones_like(Xs), v)  # (n_batch, v)
            indices = torch.randint(0, Xs.size(1), (Xs.size(0), v), device=Xs.device)
            self.hessian_approximation_ctx["indices"] = indices

        Xs.requires_grad_()

        f = get_f(self.equation, self.solution, Xs, s, self.hessian_approximation_ctx, baseline_repeat=None)

        x_baseline = tx[:, 1:]
        x_baseline.requires_grad_()
        t_baseline = tx[:, 0:1]
        f_baseline = get_f(
            self.equation,
            self.solution,
            x_baseline,
            t_baseline,
            self.hessian_approximation_ctx,
            baseline_repeat=n_estimate,
        )

        Ys = dW / torch.sqrt(s - t) / self.equation.alpha_sqrt
        # shape of eYs: (n_batch*n_estimate, 1 + nx)
        eYs = torch.cat([torch.ones_like(s), Ys], dim=-1)
        integral = (t_next - t) * (f - f_baseline) * eYs
        integral[:, 0:1] = integral[:, 0:1] + f_baseline * (t_next - t)
        integral = integral.view(n_batch, n_estimate, -1)
        integral = torch.sum(integral, dim=1, keepdim=False) / n_estimate
        return integral

    def estimate_integral_with_gradients_ou(self, tx: torch.Tensor):
        r"""
        Abandoned because now we use estimate_intrgral_with_gradients_ou_simple.
        """
        assert isinstance(self.equation, OUProcessSampleEquation)
        n_estimate = self.n_estimate_integral
        n_batch = tx.size(0)
        t, x = torch.narrow(tx, dim=-1, start=0, length=1), torch.narrow(tx, dim=-1, start=1, length=self.equation.nx)
        s = torch.rand_like(t) * (self.T - t) + t

        # joint sample with shape (n_batch, n_estimate, nx)
        Xs, intDs = self.equation.sample_Xs_int_Ds(t, s, x, n_estimate=n_estimate)
        Xs = Xs.reshape(n_batch * n_estimate, -1)
        intDs = intDs.reshape(n_batch * n_estimate, -1)
        # joint sample with shape (n_batch, n_estimate, nx)
        if self.hessian_approximation_ctx is not None:
            assert self.hessian_approximation_config.method == "SDGD"
            v = self.hessian_approximation_config.kwargs["v"]
            # indices = torch.multinomial(torch.ones_like(Xs), v)  # (n_batch, v)
            indices = torch.randint(0, Xs.size(1), (Xs.size(0), v), device=Xs.device)
            self.hessian_approximation_ctx["indices"] = indices

        # expand t, s to match the shape of Xs
        t = torch.repeat_interleave(t, n_estimate, dim=0)
        s = torch.repeat_interleave(s, n_estimate, dim=0)
        Xs.requires_grad_()

        f = get_f(self.equation, self.solution, Xs, s, self.hessian_approximation_ctx, baseline_repeat=None)

        x_baseline = tx[:, 1:]
        x_baseline.requires_grad_()
        t_baseline = tx[:, 0:1]
        f_baseline = get_f(
            self.equation,
            self.solution,
            x_baseline,
            t_baseline,
            self.hessian_approximation_ctx,
            baseline_repeat=n_estimate,
        )
        Ys = 1 / (self.equation.alpha_sqrt * (s - t)) * intDs
        # shape of eYs: (n_batch*n_estimate, 1 + nx)
        eYs = torch.cat([torch.ones_like(s), Ys], dim=-1)
        integral = (self.T - t) * (f - f_baseline) * eYs
        integral[:, 0:1] = integral[:, 0:1] + f_baseline * (self.T - t)
        integral = integral.view(n_batch, n_estimate, -1)
        integral = torch.sum(integral, dim=1, keepdim=False) / n_estimate
        return integral

    def estimate_integral_with_gradients_ou_simple(self, tx: torch.Tensor):
        r"""
        We make the assumptions for OUProcessEquation:
            - SDE: dX_t = \theta (\mu - X_t) dt + \sigma dW_t
            - Sigma = sqrt(alpha) I,
            - D = I.
        And we use the samples driven by the OU process to estimate the integral.
        ---
        The formula:
            v(t, x) = \int_t^T f(s, Xs, u(s,Xs), Sigma^T u_x(s,Xs)) (1, Ys) ds
            Ys = Sigma^T/(s-t)\int_t^s [Sigma^{-1}D_{s,r,x}]^T dWr
               = 1/(s-t)\int_t^s dWr
               = 1/sqrt(s-t) N(0,1)^d
        :param tx:
        :return:
            v(t, x): (n_batch, 1 + nx)
        """
        assert isinstance(self.equation, OUProcessSampleEquation)
        n_estimate = self.n_estimate_integral
        n_batch = tx.size(0)
        t, x = torch.narrow(tx, dim=-1, start=0, length=1), torch.narrow(tx, dim=-1, start=1, length=self.equation.nx)
        s = torch.rand_like(t) * (self.equation.T - t) + t
        # joint sample with shape (n_batch, n_estimate, nx)
        Xs, intDs = self.equation.sample_Xs_int_Ds_simple(t, s, x, n_estimate=n_estimate)
        Xs = Xs.reshape(n_batch * n_estimate, -1)
        intDs = intDs.reshape(n_batch * n_estimate, -1)

        # expand t, s to match the shape of Xs
        t = torch.repeat_interleave(t, n_estimate, dim=0)
        s = torch.repeat_interleave(s, n_estimate, dim=0)
        Xs.requires_grad_()
        f = get_f(self.equation, self.solution, Xs, s, self.hessian_approximation_ctx, baseline_repeat=None)

        x_baseline = tx[:, 1:]
        x_baseline.requires_grad_()
        t_baseline = tx[:, 0:1]
        f_baseline = get_f(
            self.equation,
            self.solution,
            x_baseline,
            t_baseline,
            self.hessian_approximation_ctx,
            baseline_repeat=n_estimate,
        )

        Ys = 1 / (self.equation.alpha_sqrt * (s - t)) * intDs
        # shape of eYs: (n_batch*n_estimate, 1 + nx)
        eYs = torch.cat([torch.ones_like(s), Ys], dim=-1)
        integral = (self.equation.T - t) * (f - f_baseline) * eYs
        integral[:, 0:1] = integral[:, 0:1] + f_baseline * (self.equation.T - t)
        integral = integral.view(n_batch, n_estimate, -1)
        integral = torch.sum(integral, dim=1, keepdim=False) / n_estimate
        return integral

    def estimate_integral_with_gradients_and_hessians(self, tx: torch.Tensor):
        r"""
        We make the assumptions:
          - Sigma = sqrt(alpha) I,
          - D = I.
        The second assumption can be derived by assuming that `\mu_x` and `\Sigma_x` are zeros.
        Therefore, the assumptions boil down to:
          - Sigma = sqrt(alpha) I,
          - `\mu_x` = 0.
        For now, SimpleDiffusionEquation satisfies the above assumptions (actually, it further requires \mu=0).
        In the future, we shall move the computation of Ys to the equation class.
        ---
        The formula:
            v(t, x) = \int_t^T f(s, Xs, u(s,Xs), Sigma^T u_x(s,Xs)) (1, Ys) ds
            Ys = Sigma^T/(s-t)\int_t^s [Sigma^{-1}D_{s,r,x}]^T dWr
               = 1/(s-t)\int_t^s dWr
               = 1/sqrt(s-t) N(0,1)^d
        :param tx:
        :return:
            v(t, x): (n_batch, 1 + nx)
        """
        assert isinstance(self.equation, SimpleDiffusionEquation)
        n_estimate = self.n_estimate_integral
        n_batch = tx.size(0)
        t, x = self.repeat_and_split(tx, n_estimate, self.equation.nx)
        s = torch.rand_like(t) * (self.T - t) + t
        t_mid = (s + t) / 2
        X_mid, dW1 = self.equation.sample_x_ts(t, t_mid, x, return_dW=True)
        Xs, dW2 = self.equation.sample_x_ts(t_mid, s, X_mid, return_dW=True)
        dW = dW1 + dW2
        Xs.requires_grad_()
        f = get_f(self.equation, self.solution, Xs, s)

        x_baseline = tx[:, 1:]
        x_baseline.requires_grad_()
        t_baseline = tx[:, 0:1]
        f_baseline_single = get_f(
            self.equation,
            self.solution,
            x_baseline,
            t_baseline,
        )
        f_baseline = torch.repeat_interleave(
            f_baseline_single,
            n_estimate,
            dim=0,
        )
        Ys = dW / torch.sqrt(s - t)
        # shape of eYs: (n_batch*n_estimate, 1 + nx)
        eYs = torch.cat([torch.ones_like(s), Ys], dim=-1)
        integral = (self.T - t) * (f - f_baseline) * eYs
        integral = integral.view(n_batch, n_estimate, -1)

        integral = torch.sum(integral, dim=1, keepdim=False) / n_estimate
        integral[:, 0:1] = integral[:, 0:1] + f_baseline_single * (self.T - t_baseline)

        integral_hessian = (
            2 * torch.sqrt(self.T - t) * (f - f_baseline) * 4 / (s - t) ** 2 / self.equation.alpha
        ).unsqueeze(-1) * torch.einsum("ij,ik->ijk", dW1, dW2)
        integral_hessian = integral_hessian.view(n_batch, n_estimate, -1).mean(dim=1)
        integral = torch.cat([integral, integral_hessian], dim=-1)
        return integral

    def estimate_integral_with_gradients_and_hessians_double_old(self, tx: torch.Tensor):
        r"""
        We make the assumptions:
          - Sigma = sqrt(alpha) I,
          - D = I.
        The second assumption can be derived by assuming that `\mu_x` and `\Sigma_x` are zeros.
        Therefore, the assumptions boil down to:
          - Sigma = sqrt(alpha) I,
          - `\mu_x` = 0.
        For now, SimpleDiffusionEquation satisfies the above assumptions (actually, it further requires \mu=0).
        In the future, we shall move the computation of Ys to the equation class.
        ---
        The formula:
            v(t, x) = \int_t^T f(s, Xs, u(s,Xs), Sigma^T u_x(s,Xs)) (1, Ys) ds
            Ys = Sigma^T/(s-t)\int_t^s [Sigma^{-1}D_{s,r,x}]^T dWr
               = 1/(s-t)\int_t^s dWr
               = 1/sqrt(s-t) N(0,1)^d
        :param tx:
        :return:
            v(t, x): (n_batch, 1 + nx)
        """
        assert isinstance(self.equation, SimpleDiffusionEquation)
        n_estimate = self.n_estimate_integral
        n_batch = tx.size(0)
        t, x = self.repeat_and_split(tx, n_estimate, self.equation.nx)
        s = torch.rand_like(t) * (self.T - t) + t + 0.0001
        t_mid = (s + t) / 2
        X_mid, dW1 = self.equation.sample_x_ts(t, t_mid, x, return_dW=True)
        Xs, dW2 = self.equation.sample_x_ts(t_mid, s, X_mid, return_dW=True)
        dW = (Xs - x) / torch.sqrt(s - t) / self.equation.alpha_sqrt
        Xs.requires_grad_()
        f_plus = get_f(self.equation, self.solution, Xs, s)

        x_baseline = tx[:, 1:]
        x_baseline.requires_grad_()
        t_baseline = tx[:, 0:1]
        f_baseline_single = get_f(
            self.equation,
            self.solution,
            x_baseline,
            t_baseline,
        )
        f_baseline = torch.repeat_interleave(
            f_baseline_single,
            n_estimate,
            dim=0,
        )
        Ys = dW / torch.sqrt(s - t)
        # shape of eYs: (n_batch*n_estimate, 1 + nx)
        eYs = torch.cat([torch.ones_like(s), Ys], dim=-1)
        integral = (self.equation.T - t) * (f_plus - f_baseline) * eYs
        integral = integral.view(n_batch, n_estimate, -1)

        integral = torch.sum(integral, dim=1, keepdim=False) / n_estimate
        integral[:, 0:1] = integral[:, 0:1] + f_baseline_single * (self.equation.T - t_baseline)

        # integral_grad = ((f_plus - f_baseline) * Ys).view(n_batch, n_estimate, -1).mean(dim=1) * (equation.T - t_baseline)
        # print(integral_grad - integral[:, 1:])

        # estimate the hessian with pos
        integral_hessian_plus = (
            (self.equation.T - t) * (f_plus - f_baseline) * 4 / ((s - t) ** 2) * (s - t) / 2
        ).unsqueeze(-1) * torch.einsum("ij,ik->ijk", dW1, dW2)
        integral_hessian_plus = integral_hessian_plus.view(n_batch, n_estimate, -1).mean(dim=1)

        # estimate the hessian with neg
        X_mid_minus = x - torch.sqrt(t_mid - t) * self.equation.alpha_sqrt * dW1
        Xs_minus = X_mid_minus - torch.sqrt(s - t_mid) * self.equation.alpha_sqrt * dW2
        Xs_minus.requires_grad_()
        f_minus = get_f(self.equation, self.solution, Xs_minus, s)
        integral_hessian_minus = (
            (self.equation.T - t) * (f_minus - f_baseline) * 4 / ((s - t) ** 2) * (s - t) / 2
        ).unsqueeze(-1) * torch.einsum("ij,ik->ijk", dW1, dW2)
        integral_hessian_minus = integral_hessian_minus.view(n_batch, n_estimate, -1).mean(dim=1)

        # estimate the hessian with pos and neg
        integral_hessian = (integral_hessian_plus + integral_hessian_minus) / 2
        integral = torch.cat([integral, integral_hessian], dim=-1)
        return integral

    def estimate_integral_with_gradients_and_hessians_double(self, tx: torch.Tensor):
        r"""
        We make the assumptions:
          - Sigma = sqrt(alpha) I,
          - D = I.
        The second assumption can be derived by assuming that `\mu_x` and `\Sigma_x` are zeros.
        Therefore, the assumptions boil down to:
          - Sigma = sqrt(alpha) I,
          - `\mu_x` = 0.
        For now, SimpleDiffusionEquation satisfies the above assumptions (actually, it further requires \mu=0).
        In the future, we shall move the computation of Ys to the equation class.
        ---
        The formula:
            v(t, x) = \int_t^T f(s, Xs, u(s,Xs), Sigma^T u_x(s,Xs)) (1, Ys) ds
            Ys = Sigma^T/(s-t)\int_t^s [Sigma^{-1}D_{s,r,x}]^T dWr
               = 1/(s-t)\int_t^s dWr
               = 1/sqrt(s-t) N(0,1)^d
        :param tx:
        :return:
            v(t, x): (n_batch, 1 + nx)
        """
        assert isinstance(self.equation, SimpleDiffusionEquation)
        n_estimate = self.n_estimate_integral
        n_batch = tx.size(0)
        t, x = self.repeat_and_split(tx, n_estimate, self.equation.nx)
        s = torch.rand_like(t) * (self.T - t) + t + 0.0001
        t_mid = (s + t) / 2
        X_mid, dW1 = self.equation.sample_x_ts(t, t_mid, x, return_dW=True)
        Xs, dW2 = self.equation.sample_x_ts(t_mid, s, X_mid, return_dW=True)
        dW = (Xs - x) / torch.sqrt(s - t) / self.equation.alpha_sqrt
        Xs.requires_grad_()
        f_plus = get_f(self.equation, self.solution, Xs, s)

        x_baseline = tx[:, 1:]
        x_baseline.requires_grad_()
        t_baseline = tx[:, 0:1]
        f_baseline_single = get_f(
            self.equation,
            self.solution,
            x_baseline,
            t_baseline,
        )
        f_baseline = torch.repeat_interleave(
            f_baseline_single,
            n_estimate,
            dim=0,
        )
        Ys = dW / torch.sqrt(s - t)
        # shape of eYs: (n_batch*n_estimate, 1 + nx)
        eYs = torch.cat([torch.ones_like(s), Ys], dim=-1)
        integral = (self.equation.T - t) * (f_plus - f_baseline) * eYs
        integral = integral.view(n_batch, n_estimate, -1)

        integral = torch.sum(integral, dim=1, keepdim=False) / n_estimate
        integral[:, 0:1] = integral[:, 0:1] + f_baseline_single * (self.equation.T - t_baseline)

        #  E_f_2 = (T-t)* (f(s,x+ W2) + f(s,x-W2)-2*f(t,x))/2 * [W2'*(W2)./(s-t).^2) - /(s-t)*eye(d);
        W2 = torch.sqrt(s - t) * torch.randn_like(x)
        Xs_plus = x + self.equation.alpha_sqrt * W2
        Xs_minus = x - self.equation.alpha_sqrt * W2
        Xs_plus.requires_grad_()
        Xs_minus.requires_grad_()
        f_plus = get_f(self.equation, self.solution, Xs_plus, s)
        f_minus = get_f(self.equation, self.solution, Xs_minus, s)

        delta_f_factor = (f_plus + f_minus - 2 * f_baseline) / 2 / (s - t)
        part_1 = (delta_f_factor / (s - t)).unsqueeze(-1) * (torch.einsum("ij,ik->ijk", W2, W2))
        part_2 = delta_f_factor.view(n_batch, n_estimate, -1).mean(dim=1, keepdim=True) * torch.eye(
            self.equation.nx, device=tx.device
        ).unsqueeze(0).repeat(n_batch, 1, 1)
        part_2 = part_2.view(n_batch, -1)
        integral_hessian = (part_1.view(n_batch, n_estimate, -1).mean(dim=1) - part_2) * (self.equation.T - t_baseline)

        integral = torch.cat([integral, integral_hessian], dim=-1)
        return integral

    def estimate_terminal_with_gradients(self, tx: torch.Tensor):
        r"""
        Assume the same assumption as in `estimate_integral_with_gradients`.
        ---
        The formula:
            E[(g(X_(t,T,x)) - g(x)) (1, Y)] + (g(x), 0)
            Y = 1/sqrt(T-t) N(0,1)^d
        :param tx:
        :return:
        """
        assert isinstance(self.equation, SimpleDiffusionEquation) or isinstance(self.equation, OUProcessEquation)
        n_estimate = self.n_estimate_terminal
        n_batch = tx.size(0)
        t, x = self.repeat_and_split(tx, n_estimate, self.equation.nx)
        T = torch.ones_like(t) * self.T
        XT, dW = self.equation.sample_x_ts(t, T, x, return_dW=True)  # (n_batch*n_estimate, 1)
        gT = self.equation.g(XT)  # (n_batch*n_estimate, 1)

        Y = dW / torch.sqrt(T - t) / self.equation.alpha_sqrt
        eY = torch.cat([torch.ones_like(gT), Y], dim=-1)  # (n_batch*n_estimate, 1+nx)
        # the faster implementation: we do NOT need
        x_single = x.view(n_batch, n_estimate, -1)[:, 0]
        g_single = self.equation.g(x_single)
        g = torch.repeat_interleave(g_single, n_estimate, dim=0)
        terminal = (gT - g) * eY
        terminal = terminal.view(n_batch, n_estimate, -1).sum(dim=1, keepdim=False) / n_estimate
        terminal[:, 0:1] = terminal[:, 0:1] + g_single
        return terminal

    def estimate_terminal_with_gradients_td(self, tx: torch.Tensor):
        r"""
        Assume the same assumption as in `estimate_integral_with_gradients` with td
        ---
        replace g(XT) with u(t_K, X_t_K)
        :param tx:
        :return:
        """
        assert isinstance(self.equation, SimpleDiffusionEquation) or isinstance(self.equation, OUProcessEquation)
        n_estimate = self.n_estimate_terminal
        n_batch = tx.size(0)
        t, x = self.repeat_and_split(tx, n_estimate, self.equation.nx)
        t_next = torch.clip(t + self.estimate_delta_t, max=self.T)
        XT, dW = self.equation.sample_x_ts(t, t_next, x, return_dW=True)
        gT = torch.where(t_next == self.T, self.equation.g(XT), self.solution(torch.cat([t_next, XT], dim=1)))
        Y = dW / torch.sqrt(t_next - t) / self.equation.alpha_sqrt
        eY = torch.cat([torch.ones_like(gT), Y], dim=-1)  # (n_batch*n_estimate, 1+nx)
        # the faster implementation: we do NOT need
        x_single = x.view(n_batch, n_estimate, -1)[:, 0]
        g_single = self.equation.g(x_single)
        g = torch.repeat_interleave(g_single, n_estimate, dim=0)
        terminal = (gT - g) * eY
        terminal = terminal.view(n_batch, n_estimate, -1).sum(dim=1, keepdim=False) / n_estimate
        terminal[:, 0:1] = terminal[:, 0:1] + g_single
        return terminal

    def estimate_terminal_with_gradients_ou(self, tx: torch.Tensor):
        r"""
        Abandoned because now we use estimate_terminal_with_gradients_ou_simple.
        """
        assert isinstance(self.equation, OUProcessSampleEquation)
        n_estimate = self.n_estimate_integral
        n_batch = tx.size(0)
        t, x = torch.narrow(tx, dim=-1, start=0, length=1), torch.narrow(tx, dim=-1, start=1, length=self.equation.nx)
        T = torch.ones_like(t) * self.T
        # joint sample with shape (n_batch, n_estimate, nx)
        XT, intDT = self.equation.sample_Xs_int_Ds(t, T, x, n_estimate=n_estimate)
        XT = XT.reshape(n_batch * n_estimate, -1)
        intDT = intDT.reshape(n_batch * n_estimate, -1)
        gT = self.equation.g(XT)  # (n_batch*n_estimate, 1)

        # expand t, x, T to match the shape of XT
        t = torch.repeat_interleave(t, n_estimate, dim=0)
        x = torch.repeat_interleave(x, n_estimate, dim=0)
        T = torch.repeat_interleave(T, n_estimate, dim=0)
        Y = 1 / (self.equation.alpha_sqrt * (T - t)) * intDT
        eY = torch.cat([torch.ones_like(gT), Y], dim=-1)  # (n_batch*n_estimate, 1+nx)
        # the faster implementation: we do NOT need
        x_single = x.view(n_batch, n_estimate, -1)[:, 0]
        g_single = self.equation.g(x_single)
        g = torch.repeat_interleave(g_single, n_estimate, dim=0)
        terminal = (gT - g) * eY
        terminal = terminal.view(n_batch, n_estimate, -1).sum(dim=1, keepdim=False) / n_estimate
        terminal[:, 0:1] = terminal[:, 0:1] + g_single
        return terminal

    def estimate_terminal_with_gradients_ou_simple(self, tx: torch.Tensor):
        r"""
        Assume the same assumption as in `estimate_integral_with_gradients_ou_simple`.
        """
        assert isinstance(self.equation, OUProcessSampleEquation)
        n_estimate = self.n_estimate_integral
        n_batch = tx.size(0)
        t, x = torch.narrow(tx, dim=-1, start=0, length=1), torch.narrow(tx, dim=-1, start=1, length=self.equation.nx)
        T = torch.ones_like(t) * self.equation.T
        XT, intDT = self.equation.sample_Xs_int_Ds_simple(t, T, x, n_estimate=n_estimate)
        XT = XT.reshape(n_batch * n_estimate, -1)
        intDT = intDT.reshape(n_batch * n_estimate, -1)
        gT = self.equation.g(XT)  # (n_batch*n_estimate, 1)
        t = torch.repeat_interleave(t, n_estimate, dim=0)
        x = torch.repeat_interleave(x, n_estimate, dim=0)
        T = torch.repeat_interleave(T, n_estimate, dim=0)
        Y = 1 / (self.equation.alpha_sqrt * (T - t)) * intDT
        eY = torch.cat([torch.ones_like(gT), Y], dim=-1)  # (n_batch*n_estimate, 1+nx)
        x_single = x.view(n_batch, n_estimate, -1)[:, 0]
        g_single = self.equation.g(x_single)
        g = torch.repeat_interleave(g_single, n_estimate, dim=0)
        terminal = (gT - g) * eY
        terminal = terminal.view(n_batch, n_estimate, -1).sum(dim=1, keepdim=False) / n_estimate
        terminal[:, 0:1] = terminal[:, 0:1] + g_single
        return terminal

    def estimate_terminal_with_gradients_ou_bygx(self, tx: torch.Tensor):
        r"""
        Directly use the gradient of g(x) to estimate the terminal.
        """
        assert isinstance(self.equation, OUProcessSampleEquation)
        n_estimate = self.n_estimate_integral
        n_batch = tx.size(0)
        t, x = torch.narrow(tx, dim=-1, start=0, length=1), torch.narrow(tx, dim=-1, start=1, length=self.equation.nx)
        T = torch.ones_like(t) * self.equation.T
        XT, intDT = self.equation.sample_Xs_int_Ds(t, T, x, n_estimate=n_estimate)
        XT = XT.reshape(n_batch * n_estimate, -1)
        XT.requires_grad_()
        with torch.enable_grad():
            gT = self.equation.g(XT)  # (n_batch*n_estimate, 1)
            g_x_T = torch.autograd.grad(
                outputs=gT.sum(),
                inputs=XT,
                create_graph=True,
                retain_graph=False,
                allow_unused=True,
            )[0]
        t = torch.repeat_interleave(t, n_estimate, dim=0)
        T = torch.repeat_interleave(T, n_estimate, dim=0)
        terminal = torch.cat([gT.detach(), g_x_T.detach() * torch.exp(t - T)], dim=-1)
        terminal = terminal.view(n_batch, n_estimate, -1).sum(dim=1, keepdim=False) / n_estimate
        return terminal

    def estimate_terminal_with_gradients_td_bygx(self, tx: torch.Tensor):
        r"""
        Directly use the gradient of g(x) to estimate the terminal.
        """
        assert isinstance(self.equation, OUProcessEquation)
        n_estimate = self.n_estimate_integral
        n_batch = tx.size(0)
        t_single, x_single = torch.narrow(tx, dim=-1, start=0, length=1), torch.narrow(
            tx, dim=-1, start=1, length=self.equation.nx
        )
        t = torch.repeat_interleave(t_single, n_estimate, dim=0)
        x = torch.repeat_interleave(x_single, n_estimate, dim=0)
        t_next = torch.clip(t + self.estimate_delta_t, max=self.T)
        x_next = self.equation.sample_x_ts(t, t_next, x, return_dW=False)
        g_x = self.equation.g_x(x_next)
        u_x = self.solution(torch.cat([t_next, x_next], dim=1))  # Calculate the solution u
        u_x = torch.zeros_like(x) if u_x is None else u_x
        g_x_replaced = torch.where(t_next == self.T, g_x, u_x).detach()

        terminal = torch.cat([torch.zeros_like(g_x[:, 0:1]), g_x_replaced], dim=1)
        terminal = terminal.view(n_batch, n_estimate, -1).sum(dim=1, keepdim=False) / n_estimate
        return terminal

    def estimate_terminal_with_gradients_and_hessians(self, tx: torch.Tensor):
        r"""
        Assume the same assumption as in `estimate_integral_with_gradients_and_hessians`.
        ---
        The formula:
            E[(g(X_(t,T,x)) - g(x)) (1, Y)] + (g(x), 0)
            Y = 1/sqrt(T-t) N(0,1)^d
        :param tx:
        :return:
        """
        assert isinstance(self.equation, SimpleDiffusionEquationWithHessian)
        n_estimate = self.n_estimate_terminal
        n_batch = tx.size(0)
        t, x = self.repeat_and_split(tx, n_estimate, self.equation.nx)
        T = torch.ones_like(t) * self.T
        t_mid = (T + t) / 2
        X_mid, dW1 = self.equation.sample_x_ts(t, t_mid, x, return_dW=True)
        XT, dW2 = self.equation.sample_x_ts(t_mid, T, X_mid, return_dW=True)
        dW = dW1 + dW2
        gT = self.equation.g(XT)
        Y = dW / torch.sqrt(T - t)
        eY = torch.cat([torch.ones_like(gT), Y], dim=-1)  # (n_batch*n_estimate, 1+nx)
        # the faster implementation: we do NOT need
        x_single = x.view(n_batch, n_estimate, -1)[:, 0]
        g_single = self.equation.g(x_single)
        g = torch.repeat_interleave(g_single, n_estimate, dim=0)
        terminal = (gT - g) * eY
        terminal = terminal.view(n_batch, n_estimate, -1).sum(dim=1, keepdim=False) / n_estimate
        terminal[:, 0:1] = terminal[:, 0:1] + g_single

        g_baseline_single = self.equation.g_x(x_single)
        g_baseline = (torch.repeat_interleave(g_baseline_single, n_estimate, dim=0) * (XT - x)).sum(1, keepdim=True)

        terminal_hessian = ((gT - g - g_baseline) * 4 / (T - t) ** 2 / self.equation.alpha).unsqueeze(
            -1
        ) * torch.einsum("ij,ik->ijk", dW1, dW2)
        terminal_hessian = terminal_hessian.view(n_batch, n_estimate, -1).mean(dim=1)
        terminal = torch.cat([terminal, terminal_hessian], dim=-1)
        return terminal

    def estimate_terminal_with_gradients_and_hessians_double_old(self, tx: torch.Tensor):
        r"""
        Assume the same assumption as in `estimate_integral_with_gradients_and_hessians`.
        ---
        The formula:
            E[(g(X_(t,T,x)) - g(x)) (1, Y)] + (g(x), 0)
            Y = 1/sqrt(T-t) N(0,1)^d
        :param tx:
        :return:
        """
        assert isinstance(self.equation, SimpleDiffusionEquationWithHessian)
        n_estimate = self.n_estimate_terminal
        n_batch = tx.size(0)
        t, x = self.repeat_and_split(tx, n_estimate, self.equation.nx)
        T = torch.ones_like(t) * self.T
        t_mid = (T + t) / 2
        X_mid, dW1 = self.equation.sample_x_ts(t, t_mid, x, return_dW=True)
        XT, dW2 = self.equation.sample_x_ts(t_mid, T, X_mid, return_dW=True)
        dW = (XT - x) / torch.sqrt(T - t) / self.equation.alpha_sqrt

        gT = self.equation.g(XT)
        Y = dW / torch.sqrt(T - t)

        eY = torch.cat([torch.ones_like(gT), Y], dim=-1)  # (n_batch*n_estimate, 1+nx)
        x_single = x.view(n_batch, n_estimate, -1)[:, 0]
        g_single = self.equation.g(x_single)
        g = torch.repeat_interleave(g_single, n_estimate, dim=0)
        terminal = (gT - g) * eY

        terminal = terminal.view(n_batch, n_estimate, -1).sum(dim=1, keepdim=False) / n_estimate
        terminal[:, 0:1] = terminal[:, 0:1] + g_single

        # estimate the terminal with the positive dW
        # E_g_1 = W2'*((g(x + W1+ W2) - g(x)).*(4*W1)/(T-t)^2)/N;
        terminal_hessian_plus = ((gT - g) * 4 / (T - t) ** 2 * (T - t) / 2).unsqueeze(-1) * torch.einsum(
            "ij,ik->ijk", dW1, dW2
        )
        terminal_hessian_plus = terminal_hessian_plus.view(n_batch, n_estimate, -1).mean(dim=1)

        # estimate the terminal with the negative dW
        X_mid_minus = x - torch.sqrt(t_mid - t) * self.equation.alpha_sqrt * dW1
        XT_minus = X_mid_minus - torch.sqrt(T - t_mid) * self.equation.alpha_sqrt * dW2
        gT_minus = self.equation.g(XT_minus)
        terminal_hessian_minus = ((gT_minus - g) * 4 / (T - t) ** 2 * (T - t) / 2).unsqueeze(-1) * torch.einsum(
            "ij,ik->ijk", dW1, dW2
        )
        terminal_hessian_minus = terminal_hessian_minus.view(n_batch, n_estimate, -1).mean(dim=1)

        # average the two hessians
        terminal_hessian = (terminal_hessian_plus + terminal_hessian_minus) / 2
        terminal = torch.cat([terminal, terminal_hessian], dim=-1)
        return terminal

    def estimate_terminal_with_gradients_and_hessians_double(self, tx: torch.Tensor):
        r"""
        Assume the same assumption as in `estimate_integral_with_gradients_and_hessians`.
        ---
        The formula:
            E[(g(X_(t,T,x)) - g(x)) (1, Y)] + (g(x), 0)
            Y = 1/sqrt(T-t) N(0,1)^d
        :param tx:
        :return:
        """
        assert isinstance(self.equation, SimpleDiffusionEquationWithHessian)
        n_estimate = self.n_estimate_terminal
        n_batch = tx.size(0)
        t, x = self.repeat_and_split(tx, n_estimate, self.equation.nx)
        T = torch.ones_like(t) * self.T
        t_mid = (T + t) / 2
        X_mid, dW1 = self.equation.sample_x_ts(t, t_mid, x, return_dW=True)
        XT, dW2 = self.equation.sample_x_ts(t_mid, T, X_mid, return_dW=True)
        dW = (XT - x) / torch.sqrt(T - t) / self.equation.alpha_sqrt

        gT = self.equation.g(XT)
        Y = dW / torch.sqrt(T - t)

        eY = torch.cat([torch.ones_like(gT), Y], dim=-1)  # (n_batch*n_estimate, 1+nx)
        x_single = x.view(n_batch, n_estimate, -1)[:, 0]
        g_single = self.equation.g(x_single)
        g = torch.repeat_interleave(g_single, n_estimate, dim=0)
        terminal = (gT - g) * eY

        terminal = terminal.view(n_batch, n_estimate, -1).sum(dim=1, keepdim=False) / n_estimate
        terminal[:, 0:1] = terminal[:, 0:1] + g_single

        # E_g_2 = W1'*((g(x+ W1) + g(x-W1)-2*g(x))/2.*W1/(T-t)^2)/N - mean((g(x+ W1) + g(x-W1)-2*g(x))/2/(T-t))*eye(d);
        W1 = torch.sqrt(T - t) * torch.randn_like(x)
        X_plus = x + self.equation.alpha_sqrt * W1
        X_minus = x - self.equation.alpha_sqrt * W1
        g_plus = self.equation.g(X_plus)
        g_minus = self.equation.g(X_minus)

        delta_g_factor = (g_plus + g_minus - 2 * g) / 2 / (T - t)  # [n_batch * n_estimate, 1]
        part_1 = (delta_g_factor / (T - t)).unsqueeze(-1) * (torch.einsum("ij,ik->ijk", W1, W1))
        part_2 = delta_g_factor.view(n_batch, n_estimate, -1).mean(dim=1, keepdim=True) * torch.eye(
            self.equation.nx, device=tx.device
        ).unsqueeze(0).repeat(n_batch, 1, 1)
        part_2 = part_2.view(n_batch, -1)
        terminal_hessian = part_1.view(n_batch, n_estimate, -1).mean(dim=1) - part_2

        terminal = torch.cat([terminal, terminal_hessian], dim=-1)
        return terminal

    def generate(self, tx: torch.Tensor):
        g = self.estimate_terminal(tx)
        y = self.estimate_integral(tx)
        return g + y

    def generate_with_gradients(self, tx: torch.Tensor):
        if self.estimate_delta_t > 0:
            if self.solution_output_dim == 1:
                g = self.estimate_terminal_with_gradients_td(tx)
            else:
                g = self.estimate_terminal_with_gradients_td_bygx(tx)
            y = self.estimate_integral_with_gradients_td(tx)
        else:
            g = self.estimate_terminal_with_gradients(tx)
            y = self.estimate_integral_with_gradients(tx)
        return g + y

    def generate_with_gradients_and_hessians(self, tx: torch.Tensor):
        g = self.estimate_terminal_with_gradients_and_hessians_double(tx)
        y = self.estimate_integral_with_gradients_and_hessians_double(tx)
        return g + y


def get_f(equation, solution, Xs, s, hessian_approximation_ctx=None, baseline_repeat=None):
    # get the output dimension of the solution
    solution_output_dim = solution(torch.zeros(1, 1 + equation.nx, device=Xs.device)).size(1)

    solution_output = solution(torch.cat([s, Xs], dim=-1))
    if equation.has_gradient_term:
        if solution_output_dim > 1:
            if solution_output_dim == 1 + equation.nx:
                u = solution_output[:, :1]
                u_x = solution_output[:, 1:]
            elif solution_output_dim == equation.nx:
                assert not equation.has_hessian_term
                assert not equation.has_laplacian_term
                u = solution_output[:, 0:1] * 0
                u_x = solution_output
            else:
                raise ValueError(f"Unknown solution_output_dim: {solution_output_dim}")
        else:
            u = solution_output
            u_x = torch.autograd.grad(
                outputs=u.sum(),
                inputs=Xs,
                create_graph=True,
                retain_graph=True,
                allow_unused=True,
            )[0]

        # Note: it is important to detach those tensors that are in the computational graph.
        #  Otherwise, Dataloader will hang when doing multiprocessing.
        # ff multiplies u_x by sqrt(alpha) internally
        if equation.has_hessian_term:
            if u_x is None:
                u_x = torch.zeros_like(Xs)
                hess_u = torch.zeros(Xs.size(0), Xs.size(1), Xs.size(1), device=Xs.device)
                f = equation.ffh(s, Xs.detach(), u.detach(), u_x.detach(), hess_u.detach())
            elif hessian_approximation_ctx is None:
                hess_u = torch.zeros(Xs.size(0), Xs.size(1), Xs.size(1), device=Xs.device)
                for i in range(u_x.size(1)):
                    grad_grad = torch.autograd.grad(
                        u_x[:, i],
                        Xs,
                        grad_outputs=torch.ones_like(u_x[:, i]),
                        create_graph=True,
                        only_inputs=True,
                    )[0]
                    hess_u[:, i, :] = grad_grad
                f = equation.ffh(s, Xs.detach(), u.detach(), u_x.detach(), hess_u.detach())
            else:
                indices = hessian_approximation_ctx["indices"]
                indices_effective = indices
                if baseline_repeat is not None:
                    indices_effective = torch.arange(Xs.size(1), device=Xs.device).repeat(Xs.size(0), 1)
                # indices_effective has shape (N*M, v)
                u_ii_all = torch.zeros(indices_effective.size(), device=Xs.device)
                for i in range(indices_effective.size(1)):
                    active_index = indices_effective[:, i].unsqueeze(dim=-1)  # shape: (n_batch*n_estimate,)
                    # u_ii's shape: (n_batch*n_estimate, nx)
                    active_u_x = torch.gather(u_x, 1, active_index)
                    u_ii = torch.autograd.grad(
                        active_u_x,
                        Xs,
                        grad_outputs=torch.ones_like(active_u_x),
                        create_graph=True,
                        only_inputs=True,
                    )[0]
                    # get the active u_ii
                    u_ii_all[:, i] = torch.gather(u_ii, 1, active_index).squeeze()
                if baseline_repeat is None:
                    f = equation.ffi(s, Xs.detach(), u.detach(), u_ii_all.detach())
                else:
                    # Computing the baseline
                    u_ii_all = torch.repeat_interleave(u_ii_all, baseline_repeat, dim=0)
                    u_ii_all_gathered = torch.gather(u_ii_all, 1, indices)  # shape: (n_batch*n_repeat, v)
                    u_repeated = torch.repeat_interleave(u, baseline_repeat, dim=0)
                    s_repeated = torch.repeat_interleave(s, baseline_repeat, dim=0)
                    Xs_repeated = torch.repeat_interleave(Xs, baseline_repeat, dim=0)
                    f = equation.ffi(s_repeated, Xs_repeated.detach(), u_repeated.detach(), u_ii_all_gathered.detach())
                    return f  # prevent being repeated twice
        elif equation.has_laplacian_term:
            if u_x is None:
                u_x = torch.zeros_like(Xs)
                u_xx = torch.zeros_like(u)
            else:
                if equation.num_v_samples > 0:
                    u_xx = hutchinson_trace_estimation_batch(solution, s, Xs, equation.num_v_samples)
                else:
                    u_xx = get_laplacian(Xs, u, u_x)

            f = equation.ffl(s, Xs.detach(), u.detach(), u_x.detach(), u_xx.detach())
        else:
            u_x = u_x if u_x is not None else torch.zeros_like(Xs)
            f = equation.ff(s, Xs.detach(), u.detach(), u_x.detach())  # shape: (n_batch*n_estimate, 1)
    else:
        u = solution_output
        f = equation.f(s, Xs.detach(), u.detach())
    if baseline_repeat is not None:
        # Computing the baseline
        f = torch.repeat_interleave(f, baseline_repeat, dim=0)

    return f


class TwoLayerOnlineDataGenerator(_OnlineDataGenerator):
    def generate_with_gradients_and_hessians(self, tx: torch.Tensor) -> torch.Tensor:
        raise NotImplementedError

    def generate_with_gradients(self, tx: torch.Tensor) -> torch.Tensor:
        raise NotImplementedError

    def __init__(
        self,
        equation: Equation,
        solution: torch.nn.Module,
        solution_m2: torch.nn.Module,
        N: int,
        i: int,
        *,
        device: Union[str, torch.device] = torch.device("cpu"),
        n_estimate_residual=1,
        t_always_uniform=False,
    ):
        super().__init__(equation, N, i, device=device, t_always_uniform=t_always_uniform)
        self.solution = solution
        self.solution_m2 = solution_m2
        self.to(device=device)
        self.n_estimate_residual = n_estimate_residual

    def to(self, device):
        super().to(device=device)
        self.solution = self.solution.to(device=device)
        self.solution_m2 = self.solution_m2.to(device=device)
        return self

    def estimate_residual(self, tx: torch.Tensor):
        r"""
        estimate E\int_t^T f(s, Xs, u1(s,Xs)) - f(s, Xs, u2(s,Xs)) ds
        :param tx: (n_batch, 1+nx)
        :return:
        """
        n_batch = tx.size(0)
        n_estimate = self.n_estimate_residual
        t, s, Xs, sXs = self.generate_sx_for_integral(tx, n_estimate)
        u1 = self.solution(sXs)
        u2 = self.solution_m2(sXs)
        f1 = self.equation.f(s, Xs, u1)
        f2 = self.equation.f(s, Xs, u2)
        integral = (f1 - f2) * (self.T - t)
        integral = integral.view(n_batch, n_estimate, self.equation.nu)
        integral = torch.sum(integral, dim=1, keepdim=False) / n_estimate
        return integral

    def generate(self, tx: torch.Tensor) -> torch.Tensor:
        u = self.solution(tx)
        return u + self.estimate_residual(tx)


class OfflineDataGenerator(DataGenerator):
    do_internal_batching = False

    def __init__(
        self,
        *,
        train_file: str,
        device: Union[str, torch.device] = torch.device("cpu"),
    ):
        self.data = np.load(train_file)
        self.data = torch.from_numpy(self.data).float()
        self.data = self.data.to(device)

    def sample(self, n_batch):
        idx = torch.randint(0, self.data.size(0), (n_batch,))
        tx = self.data[idx, :-1]
        u = self.data[idx, -1:]
        return tx, u

    def to(self, device):
        self.data = self.data.to(device)
        return self

    def dataset(self, n_total: int, n_batch_buffer: float, batch_size: int) -> Dataset:
        if abs(n_batch_buffer) >= 1e-5:
            warnings.warn("OfflineDataGenerator does not support internal batching, " "so n_batch_buffer is ignored")
        return TensorDataset(*self.sample(n_total))


class PicardDataModule(pl.LightningDataModule):
    def __init__(
        self,
        equation: Equation,
        solution: torch.nn.Module,
        N: int,
        i: int,
        data_cfg: CfgNode,
        batch_size: int,
        exp_dir: pathlib.Path = None,
        solution_m2: torch.nn.Module = None,
        generate_gradients=False,
        generate_hessians=False,
        do_multi_epochs=False,
        dataset_size_info_args=None,
    ):
        """
        :param equation:
        :param solution:
        :param N:
        :param i:
        :param data_cfg:
        :param batch_size: if set to 0, will use the whole dataset as a batch
        :param exp_dir:
        :param solution_m2:
        :param generate_gradients: whether to generate gradients or not
        :param generate_hessians: whether to generate hessians or not
        :param do_multi_epochs:
            set this to True if you want to do multiple epochs.
            The online data generator does not cache the data by default,
             set this to True to cache the data to file to enable multiple epochs.
            Currently, shuffle is not supported for multiple epochs.
        """
        super().__init__()
        self.batch_size = batch_size
        self.equation = equation
        self.data_cfg = data_cfg
        self.generate_gradients = generate_gradients
        self.generate_hessians = generate_hessians
        self._device = get_device_prefer_cuda(data_cfg.DEVICE)
        self.do_multi_epochs = do_multi_epochs
        self.data_dim_input = 1 + self.equation.nx
        self.data_name_input = "tx"
        self.data_dim = 1
        self.data_name = "u"
        self.data_generator, self.data_dir = self.get_data_generator(exp_dir, N, i, solution, solution_m2)
        # active_data_size is the size of the data that is actually being sampled
        self.active_data_size = self.data_cfg.DATA_SIZE  # DATA_SIZE might be adjusted runtime
        self.dataset_size_info_args = dataset_size_info_args

        # actually, val_dataloader is invoked before train_dataloader
        warnings.filterwarnings("ignore", r".*workers.*bottleneck.*num_workers.*", PossibleUserWarning)
        warnings.filterwarnings("ignore", r".*IterableDataset.*__len__.*inaccurate.*", UserWarning)

    def get_data_generator(self, exp_dir: Optional[pathlib.Path], N: int, i: int, solution, solution_m2):
        should_save_data_to_file = self.data_cfg.SAVE or self.do_multi_epochs or self.data_cfg.PRELOAD
        if should_save_data_to_file:
            assert exp_dir is not None
            data_dir = self.get_data_save_folder(exp_dir, i)
            data_dir.mkdir(parents=True, exist_ok=True)
        else:
            data_dir = None
        if self.data_cfg.ONLINE:
            kws = dict(
                equation=self.equation,
                solution=solution,
                N=N,
                i=i,
                device=self._device,
                **self.data_cfg.kwargs,
            )
            if self.data_cfg.HESSIAN_APPROXIMATION is not None:
                kws["hessian_approximation"] = self.data_cfg.HESSIAN_APPROXIMATION
            if self.data_cfg.SAMPLE_BOUND is not None:
                kws["sample_bound"] = self.data_cfg.SAMPLE_BOUND
            kws["estimate_terminal"] = self.data_cfg.ESTIMATE_TERMINAL
            kws["estimate_integral"] = self.data_cfg.ESTIMATE_INTEGRAL
            kws["estimate_delta_t"] = self.data_cfg.ESTIMATE_DELTA_T
            data_generator = (
                TwoLayerOnlineDataGenerator(solution_m2=solution_m2, **kws)
                if solution_m2 is not None
                else OnlineDataGenerator(**kws)
            )
        else:
            data_generator = OfflineDataGenerator(train_file=self.data_cfg.TRAIN_FILE, device=self._device)
        return data_generator, data_dir

    @staticmethod
    def get_data_save_folder(exp_dir: pathlib.Path, i: int):
        return exp_dir / f"data_iter_{i}"

    def to(self, device):
        self._device = get_device_prefer_cuda(device)
        self.data_generator.to(device=self._device)
        return self

    def get_data_files(self) -> List[pathlib.Path]:
        return list(self.data_dir.glob("split_*.h5"))

    def initialize_dataset(self, dataset, worker_id, num_workers: int):
        print("Initializing dataset...")
        n_total_data_this_worker = self.active_data_size // num_workers
        h5_config = tuple()
        if self.data_cfg.SAVE:
            h5_config = (
                self.data_dir / f"split_{worker_id:02d}.h5",
                n_total_data_this_worker,
                [self.data_dim_input, self.data_dim],
                [self.data_name_input, self.data_name],
                get_standard_float_type_np(self.data_cfg.FLOAT),
            )
        if isinstance(dataset, IterableDatasetWithInternalBatch):
            dataset.set_size(n_total_data_this_worker)
            if self.data_cfg.SAVE:
                dataset.attach_saver(H5Saver(*h5_config))
        elif isinstance(dataset, TensorDataset):
            raise NotImplementedError("TensorDataset is not supported for now!")
        elif isinstance(dataset, CacheToFileWrapper):
            dataset.init(*h5_config, preload=self.data_cfg.PRELOAD)
        elif isinstance(dataset, CacheToMemoryWrapper):
            if self.data_cfg.SAVE:
                print("Adding saver info to CacheToMemoryWrapper dataset")
                dataset.enable_save_to_file(h5_config)
            dataset.init(
                n_total_data_this_worker,
                [1 + self.equation.nx, self.data_dim],
                preload=self.data_cfg.PRELOAD,
            )
        else:
            raise NotImplementedError(f"Unknown dataset type {type(dataset)}, multi-processing is not supported")

    def dataset_init_fn(self, worker_id):  # noqa
        # DataLoader does not preserve the default dtype of the worker process; see:
        # https://github.com/pytorch/vision/issues/3393
        # https://github.com/pytorch/pytorch/issues/31689
        worker_info = get_worker_info()
        set_default_dtype(self.data_cfg.FLOAT)
        dataset = worker_info.dataset
        self.initialize_dataset(dataset, worker_id, worker_info.num_workers)

    def estimate_n_buffer_per_worker(self, dataset_fn):
        memory_config = self.data_cfg.MEMORY
        extra_reduce_factor = memory_config.REDUCE_FACTOR
        if self.data_cfg.N_BUFFER is not None and abs(self.data_cfg.N_BUFFER) > 1e-5:
            # if N_BUFFER is set, we use it directly
            return self.data_cfg.N_BUFFER
        n_worker = max(self.data_cfg.N_WORKERS, 1)
        total_number_of_batches_per_worker_to_be_sampled = self.active_data_size // self.batch_size // n_worker
        assert (
            total_number_of_batches_per_worker_to_be_sampled * self.batch_size * n_worker == self.active_data_size
        ), "DATA_SIZE must be a multiple of BATCH_SIZE * N_WORKERS"
        if self.data_cfg.N_BUFFER == 0:
            # means sample the whole dataset
            return total_number_of_batches_per_worker_to_be_sampled

        if not torch.cuda.is_available():
            raise ValueError("CUDA is nto available. Cannot determine n_buffer, please set DATA.N_BUFFER.")
        # estimate memory usage
        estimation_result = GPUMemoryTracker.estimate_memory_usage(dataset_fn, batch_size=self.batch_size)
        mb, n_buffer = estimation_result.peak_memory_usage, estimation_result.n_buffer
        print(f"Maximal usage of GPU memory: {mb} MB for sampling {n_buffer} batches data")
        memory_allocated_torch_mb = GPUMemoryTracker.get_memory_allocated_by_torch()
        memory_free_total_mb = GPUMemoryTracker.get_memory_available()
        # for simplicity, we assume allocated memory will be copied to other workers
        memory_available_mb = memory_free_total_mb - memory_allocated_torch_mb * (n_worker - 1)
        memory_reserved_mb = memory_config.RESERVED
        memory_available_mb = memory_available_mb - memory_reserved_mb
        memory_available_mb_per_worker = memory_available_mb / n_worker
        memory_per_batch_mb = mb / n_buffer
        maximal_n_buffer_per_worker = min(
            memory_available_mb_per_worker / memory_per_batch_mb / extra_reduce_factor,
            total_number_of_batches_per_worker_to_be_sampled,
        )

        # max n_buffer_per_worker
        # under constraints:
        #   total_number_of_batches_per_worker / n_buffer_per_worker is an integer
        #   n_buffer_per_worker <= maximal_n_buffer_per_worker
        if maximal_n_buffer_per_worker < 1:
            #  we require that 1 / n_buffer_per_worker is an integer
            # with extra constraint: batch_size * n_buffer_per_worker is an integer
            n_buffer_per_worker = None
            for n_calls_in_a_batch in divisors(round(self.batch_size)):
                if n_calls_in_a_batch >= 1 / maximal_n_buffer_per_worker:
                    n_buffer_per_worker = 1 / n_calls_in_a_batch
                    n_iter = total_number_of_batches_per_worker_to_be_sampled / n_buffer_per_worker
                    if abs(round(n_iter) - n_iter) < 1e-5:
                        break
            assert n_buffer_per_worker is not None, (
                f"Cannot find n_buffer_per_worker such that "
                f"total_number_of_batches_per_worker {total_number_of_batches_per_worker_to_be_sampled} "
                f"/ n_buffer_per_worker is an integer "
                f", n_buffer_per_worker <= maximal_n_buffer_per_worker {maximal_n_buffer_per_worker}, "
                f" and batch_size {self.batch_size} * n_buffer_per_worker is an integer"
            )

        else:
            least_n_iter_to_be_sampled = ceil(
                total_number_of_batches_per_worker_to_be_sampled / maximal_n_buffer_per_worker
            )
            # total / n_iter = n_buffer_per_worker should be an integer; n_iter >= least_n_...
            # find integers n_iter * n_buffer = total with n_iter >= least_n_iter
            n_buffer_per_worker = 1
            for d in divisors(round(total_number_of_batches_per_worker_to_be_sampled)):
                if d >= least_n_iter_to_be_sampled:
                    n_buffer_per_worker = total_number_of_batches_per_worker_to_be_sampled // d
                    break
        return n_buffer_per_worker

    def get_dataset_details(self):
        _f = []
        if self.data_cfg.EXACT:
            _f.append("exact")
        if self.generate_gradients:
            _f.append("gradient")
        if self.generate_hessians:
            _f.append("hessian")

        Details = namedtuple("Details", ["dataset_fn", "data_dim", "data_name"])
        return {
            "exact+gradient": Details(
                self.data_generator.dataset_exact_with_gradients,
                1 + self.equation.nx,
                "u_ux",
            ),
            "exact+gradient+hessian": Details(
                self.data_generator.dataset_exact_with_gradients_and_hessians,
                1 + self.equation.nx + self.equation.nx**2,
                "u_ux_uh",
            ),
            "exact": Details(
                self.data_generator.dataset_exact,
                1,
                "u",
            ),
            "gradient": Details(
                self.data_generator.dataset_with_gradients,
                1 + self.equation.nx,
                "u_ux",
            ),
            "gradient+hessian": Details(
                self.data_generator.dataset_with_gradients_and_hessians,
                1 + self.equation.nx + self.equation.nx**2,
                "u_ux_uh",
            ),
            "": Details(
                self.data_generator.dataset,
                1,
                "u",
            ),
        }["+".join(_f)]

    def get_dataset_size_info_args(self, dataset_fn):
        title = f"Data Generation Size Info ({dataset_fn})"
        keys = [
            "Total number of data in sampling",
            "Number of batch in one generate call",
            "Number of data in one batch",
        ]
        if self.dataset_size_info_args is not None:
            self.active_data_size = self.dataset_size_info_args[0]
            # convert two tuples to a dict
            rprint(Panel(f"Re-using dataset size info: {dict(zip(keys, self.dataset_size_info_args))}", title=title))
            return self.dataset_size_info_args

        if not self.data_cfg.NEW_SAMPLING:
            n_buffer_per_worker = self.estimate_n_buffer_per_worker(dataset_fn)
            dataset_size_info_args = (self.active_data_size, n_buffer_per_worker, self.batch_size)
            rprint(
                Panel(
                    f"dataset size info: {dict(zip(keys, dataset_size_info_args))}\n"
                    f"Number of workers: {self.data_cfg.N_WORKERS}",
                    title=title,
                )
            )

        else:
            assert self.data_cfg.N_WORKERS <= 1, "it make no sense to use multiple workers"
            estimate_results = GPUMemoryTracker.estimate_largest_data_points(
                dataset_fn, self.data_cfg.MEMORY.RESERVED, n_data_per_sample_init=min(1024, self.data_cfg.DATA_SIZE)
            )
            n_data_per_sample = round(estimate_results.n_data_per_sample / self.data_cfg.MEMORY.REDUCE_FACTOR)
            n_total_data_requested = self.data_cfg.DATA_SIZE
            n_total_sample_times = ceil(n_total_data_requested / n_data_per_sample)
            # To make sure the extra data does not increase the number of training batches
            n_data_per_sample_preserve_num_training_batches = ceil(n_total_data_requested / n_total_sample_times)
            n_total_data_sampled = n_total_sample_times * n_data_per_sample_preserve_num_training_batches
            self.active_data_size = n_total_data_sampled
            dataset_size_info_args = (self.active_data_size, 1, n_data_per_sample_preserve_num_training_batches)

            rprint(
                Panel(
                    f"dataset size info: {dict(zip(keys, dataset_size_info_args))}\n"
                    f"(Original sample batch = {n_data_per_sample} using {estimate_results.peak_memory_usage} MB)\n"
                    f"Sampling {n_total_data_sampled} data points in {n_total_sample_times} times:"
                    f" resulted in {n_total_data_sampled // self.batch_size} batches\n"
                    f"Requested data size:"
                    f" {n_total_data_requested} = {n_total_data_requested // self.batch_size} batches",
                    title=title,
                )
            )
        return dataset_size_info_args

    def get_dataset_and_set_data_info(self):
        dataset_fn, self.data_dim, self.data_name = self.get_dataset_details()
        self.dataset_size_info_args = self.get_dataset_size_info_args(dataset_fn)
        dataset = dataset_fn(*self.dataset_size_info_args)
        if self.dataset_size_info_args[-1] != self.batch_size:
            print(
                "Batch size in training is different from the batch size in data generation\n"
                "Will wrap the dataset to match the batch size in training."
            )
            memory_enough = self.is_memory_enough()
            if not memory_enough:
                raise RuntimeError("Memory is not enough to sample the whole dataset")
            dataset = CacheToMemoryWrapper(
                dataset,
                batch_size=self.batch_size,
                drop_last=True,
                shuffle=self.data_cfg.SHUFFLE,
            )

        return dataset

    def is_memory_enough(self):
        total_data_in_bytes = (
            self.active_data_size * (self.data_dim_input + self.data_dim)  # input, output
        ) * get_float_type_bytes(self.data_cfg.FLOAT)
        # get total memory
        a_small_amount_of_memory_reserved = 1 * 1024 * 1024 * 1024  # 1 GB
        total_memory_in_bytes = psutil.virtual_memory().available - a_small_amount_of_memory_reserved
        print(f"The whole dataset takes {total_data_in_bytes / 1024 / 1024 / 1024} GB memory.")
        memory_ok = total_memory_in_bytes > total_data_in_bytes
        return memory_ok

    def wrap_dataset(self, dataset) -> IterableDataset:
        if isinstance(dataset, CacheToMemoryWrapper):
            return dataset
        if self.do_multi_epochs or self.data_cfg.PRELOAD:
            assert isinstance(self.data_generator, _OnlineDataGenerator)
            assert isinstance(dataset, IterableDatasetWithInternalBatch)
            if self.is_memory_enough():
                print("Total memory is enough to cache the data...Use in memory cache")
                dataset = CacheToMemoryWrapper(dataset, shuffle=self.data_cfg.SHUFFLE)
            else:
                print("Total memory is not enough to cache the data...Cache to file")
                assert not self.data_cfg.SHUFFLE
                dataset = CacheToFileWrapper(dataset)
            return dataset
        return dataset

    def train_dataloader(self):
        # Ultimately, the lightning Trainer will get the length of the dataloader by calling `len(data)`.
        # In the `__len__` method of the dataloader, it will call `len(self.dataset)`.
        # Therefore, we need to make sure that `len(self.dataset)` returns the correct length.
        dataset = self.get_dataset_and_set_data_info()
        dataset = self.wrap_dataset(dataset)
        if self.data_cfg.N_WORKERS == 0:
            self.initialize_dataset(dataset, 0, 1)
        worker_should_persistent = self.data_cfg.N_WORKERS > 0 and self.do_multi_epochs

        loader = DataLoader(
            dataset,
            batch_size=(None if self.data_generator.do_internal_batching else self.batch_size),
            num_workers=self.data_cfg.N_WORKERS,
            prefetch_factor=self.data_cfg.PREFETCH_FACTOR,
            worker_init_fn=self.dataset_init_fn,
            persistent_workers=worker_should_persistent,
        )
        return loader

    def val_dataloader(self) -> EVAL_DATALOADERS:
        return DataLoader(DummyDataset(), batch_size=None)
